{
  "hash": "4a80b3961811b842432d758662935bc5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linux & the Shell — Open-Ended Exercises\"\nauthor: \"Advanced Data Science\"\ndate: \"August 20, 2025\"\nformat:\n  html:\n    code-fold: true\neditor: source\nengine: knitr\n---\n\n\n\n> **Audience:** PhD-level biostatisticians. Focus on workflows for data science, automation, compression, and reproducibility.\n>\n> **Tip:** Unless stated otherwise, exercises assume a CSV named `penguins.csv` (with a header) in the working directory. Exercise **0** shows how to download one from the internet. Answers are hidden—click to reveal.\n\n---\n\n## 0) Download a CSV from the internet (and name it `penguins.csv`)\n**Question.** Use a command-line tool to download a CSV and save it as `penguins.csv`. Verify it looks like a CSV and preview the first few lines.\n\nThe file is available at https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n\n\n<details><summary>Show solution</summary>\n\n```bash\n# Using curl (follow redirects, write to file):\ncurl -L -o penguins.csv \\\n  https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n# Or using wget:\nwget -O penguins.csv \\\n  https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n# (Optional) R from shell:\nR -q -e \"download.file('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv','penguins.csv', mode='wb')\"\n\n# (Optional) Python from shell:\npython - <<'PY'\nimport urllib.request\nurl='https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv'\nurllib.request.urlretrieve(url, 'penguins.csv')\nPY\n\n# Basic checks\nfile penguins.csv\nhead -n 5 penguins.csv\nwc -l penguins.csv   # total lines (incl. header)\n```\n\nNotes:\n- `-L` (curl) follows redirects. `-O/-o` choose output filename.\n- Use `head`, `wc -l`, and `cut -d',' -f1-5 | head` to quickly sanity-check.\n</details>\n\n---\n\n## 1) Where am I? What’s here?\n**Question.** Print your current directory and list files with sizes and hidden entries.\n\n<details><summary>Show solution</summary>\n\n```bash\npwd\nls -lha\n```\n</details>\n\n---\n\n## 2) Create a working area\n**Question.** Make a folder `shell_practice` and change into it. Create `notes.md`.\n\n<details><summary>Show solution</summary>\n\n```bash\nmkdir -p shell_practice && cd shell_practice\n: > notes.md   # or: touch notes.md\n```\n</details>\n\n---\n\n## 3) Count rows in an **uncompressed** CSV (skip header)\n**Question.** Count the number of data rows (exclude the header line) in `penguins.csv`.\n\n<details><summary>Show solution</summary>\n\n```bash\n# total lines minus header\ntotal=$(wc -l < penguins.csv)\necho $(( total - 1 ))\n\n# or with tail:\ntail -n +2 penguins.csv | wc -l\n```\n</details>\n\n---\n\n## 4) Compress a CSV with `gzip` and `pigz`\n**Question.** Create `penguins.csv.gz` using (a) `gzip` and (b) `pigz`. Compare time and file size.\n\n<details><summary>Show solution</summary>\n\n```bash\n# (a) Using gzip\ntime gzip -kf penguins.csv     # -k keep original, -f overwrite\nls -lh penguins.csv penguins.csv.gz\n\n# (b) Using pigz (parallel gzip)\n# If missing, install via your package manager (e.g., apt, brew, conda).\ntime pigz -kf penguins.csv\nls -lh penguins.csv penguins.csv.gz\n\n# Inspect compressed vs uncompressed byte counts\ngzip -l penguins.csv.gz\n```\n\nNotes:\n- `pigz` uses multiple cores → faster on large files; compression ratio is the same algorithm as gzip.\n</details>\n\n---\n\n## 5) Count rows in a **compressed** CSV (`.gz`)\n**Question.** Count data rows in `penguins.csv.gz` **without** fully decompressing to disk.\n\n<details><summary>Show solution</summary>\n\n```bash\n# Using gzip’s decompressor:\ngzip -cd penguins.csv.gz | tail -n +2 | wc -l\n\n# Using pigz if available:\npigz -dc penguins.csv.gz | tail -n +2 | wc -l\n\n# Using zcat (often symlinked to gzip -cd):\nzcat penguins.csv.gz | tail -n +2 | wc -l\n```\n\nWhy: `-c` writes to stdout; `-d` decompresses. `tail -n +2` skips the header.\n</details>\n\n---\n\n## 6) Quick column exploration with `cut`, `head`, `sort`, `uniq`\n**Question.** Show the first 5 IDs and the distinct sex values with counts.\n\n<details><summary>Show solution</summary>\n\n```bash\nhead -n 6 penguins.csv | cut -d',' -f1      # header + first 5 IDs\ncut -d',' -f2 penguins.csv | tail -n +2 | sort | uniq -c\n```\n</details>\n\n---\n\n## 7) Filter rows by a condition with `awk`\n**Question.** Count participants with `age >= 60`. Compute the mean BMI.\n\n<details><summary>Show solution</summary>\n\n```bash\n# age >= 60 (age is 3rd column)\nawk -F',' 'NR>1 && $3 >= 60 {c++} END{print c+0}' penguins.csv\n\n# mean BMI (4th column)\nawk -F',' 'NR>1 {s+=$4; n++} END{print s/n}' penguins.csv\n```\n</details>\n\n---\n\n## 8) Find rows with missing values in **any** field\n**Question.** Count how many data rows contain an empty field.\n\n<details><summary>Show solution</summary>\n\n```bash\n# simple heuristic: consecutive delimiters or trailing comma\ngrep -E ',,' penguins.csv | wc -l\n# more thorough (detect empty at start/end or middle):\nawk -F',' 'NR>1{for(i=1;i<=NF;i++) if($i==\"\"){m++ ; break}} END{print m+0}' penguins.csv\n```\n</details>\n\n---\n\n## 9) Save the first 20 IDs to a file\n**Question.** Write the first 20 **IDs** (not including header) to `sample_ids.txt`.\n\n<details><summary>Show solution</summary>\n\n```bash\ntail -n +2 penguins.csv | cut -d',' -f1 | head -n 20 > sample_ids.txt\nwc -l sample_ids.txt   # should be 20\n```\n</details>\n\n---\n\n## 10) Chain operations with pipes\n**Question.** Among *male* participants, show counts by **age** (3rd column) in ascending order.\n\n<details><summary>Show solution</summary>\n\n```bash\nawk -F',' 'NR>1 && $2==\"male\"{print $3}' penguins.csv | sort -n | uniq -c\n```\n</details>\n\n---\n\n## 11) Make the analysis reproducible with a script\n**Question.** Create `analyze.sh` that prints: total rows, rows age ≥ 60, and mean BMI. Run it.\n\n<details><summary>Show solution</summary>\n\n```bash\ncat > analyze.sh << 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\ncsv=\"${1:-penguins.csv}\"\n\necho \"File: $csv\"\necho -n \"Total data rows: \"\ntail -n +2 \"$csv\" | wc -l\n\necho -n \"Age >= 60 rows: \"\nawk -F',' 'NR>1 && $3 >= 60 {c++} END{print c+0}' \"$csv\"\n\necho -n \"Mean BMI: \"\nawk -F',' 'NR>1 {s+=$4; n++} END{print s/n}' \"$csv\"\nEOF\n\nchmod +x analyze.sh\n./analyze.sh penguins.csv\n```\n</details>\n\n---\n\n## 12) Script for **compressed** input\n**Question.** Modify your script so it also accepts `penguins.csv.gz` seamlessly.\n\n<details><summary>Show solution</summary>\n\n```bash\ncat > analyze_any.sh << 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\nf=\"${1:-penguins.csv}\"\n\nstream() {\n  case \"$f\" in\n    *.gz)  gzip -cd \"$f\" ;;\n    *)     cat \"$f\" ;;\n  esac\n}\n\n# Skip header once:\ndata=\"$(stream | tail -n +2)\"\n\necho \"File: $f\"\necho \"Total data rows: $(printf \"%s\\n\" \"$data\" | wc -l)\"\necho \"Age >= 60 rows: $(printf \"%s\\n\" \"$data\" | awk -F',' '$3>=60{c++} END{print c+0}')\"\necho \"Mean BMI: $(printf \"%s\\n\" \"$data\" | awk -F',' '{s+=$4; n++} END{print s/n}')\"\nEOF\n\nchmod +x analyze_any.sh\n./analyze_any.sh penguins.csv.gz\n```\n</details>\n\n---\n\n## 13) Record a reproducible terminal session\n**Question.** Record your workflow to `session.log` and preview it.\n\n<details><summary>Show solution</summary>\n\n```bash\nscript -q session.log\n# …run a few commands…\nexit\nless session.log\n```\n</details>\n\n---\n\n## 14) One-liners for large files\n**Question.** Show the **uncompressed byte size** of `penguins.csv.gz` without fully inflating it; then estimate memory needed to load the CSV.\n\n<details><summary>Show solution</summary>\n\n```bash\n# Uncompressed and compressed sizes (bytes):\ngzip -l penguins.csv.gz\n\n# Rough row count without header (streaming):\nrows=$(gzip -cd penguins.csv.gz | tail -n +2 | wc -l)\necho \"Rows: $rows\"\n```\n\nNotes:\n- `gzip -l` reports compressed and uncompressed sizes; not row count.\n- Memory needs depend on parsing overhead; this is only an order-of-magnitude check.\n</details>\n\n---\n\n## 15) Remote/HPC touchpoint (optional)\n**Question.** Copy your CSV to a remote machine and check its line count there.\n\n<details><summary>Show solution</summary>\n\n```bash\nscp penguins.csv user@server:~/data/\nssh user@server 'wc -l ~/data/penguins.csv'\n```\n</details>\n\n---\n\n## 16) Parallel compression benchmarking (optional, larger files)\n**Question.** Compare wall-clock time for `gzip` vs `pigz` on a large file.\n\n<details><summary>Show solution</summary>\n\n```bash\n# Create a larger file by duplication (demo only):\nawk 'NR==1 || FNR>1' penguins.csv penguins.csv penguins.csv penguins.csv penguins.csv \\\n  > big.csv   # header from first file, rest skip header\n\n# Benchmark (prints elapsed time)\n/usr/bin/time -f \"gzip: %E\" gzip -kf big.csv\n/usr/bin/time -f \"pigz: %E\" pigz -kf big.csv\nls -lh big.csv big.csv.gz\n```\n</details>\n\n---\n\n## 17) Sanity checks and integrity\n**Question.** Verify that the compressed and uncompressed files have identical content checksums.\n\n<details><summary>Show solution</summary>\n\n```bash\nmd5sum penguins.csv\ngzip -c penguins.csv | md5sum       # checksum of compressed stream (different)\ngzip -cd penguins.csv.gz | md5sum   # checksum of decompressed content stream\n# To compare content equality:\nmd5sum penguins.csv > a.md5\ngzip -cd penguins.csv.gz | md5sum > b.md5\ndiff a.md5 b.md5   # no output => identical content\n```\n\nExplanation: the compressed file’s checksum differs, but the **decompressed content** checksum should match the original.\n</details>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}