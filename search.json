[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "",
    "text": "Welcome to Advanced Data Science! This course will focus on hands-on data analyses with a main objective of solving real-world problems, working with data science technology, and filling in gaps for real-world work as a biostatistical data scientist.\nWe will teach the necessary skills to gather, manage, and analyze data mainly using the R programming language.\nThe course will cover an introduction to data wrangling, exploratory data analysis, statistical inference and modeling, machine learning, and high-dimensional data analysis.\nWe will teach the necessary skills to develop data products including reproducible reports that can be used to effectively communicate results from data analyses. We will train students to become data scientists capable of both applied data analysis and critical evaluation of the next generation next generation of statistical methods."
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nThis course is designed for PhD students in the Biostatistics department at Johns Hopkins Bloomberg School of Public Health. It assumes a fair amount of statistical knowledge and moves relatively quickly. We are open to anyone taking the class, but since it is a core requirement for our PhD program we will not be slowing down or allowing auditors for the class."
  },
  {
    "objectID": "index.html#required-textbook",
    "href": "index.html#required-textbook",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\nNone. Instead, we have a list of recommended readings on the web site available at Resources."
  },
  {
    "objectID": "index.html#course-communication",
    "href": "index.html#course-communication",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "Course Communication",
    "text": "Course Communication\nWe will use Slack/CoursePlus to organize course discussions. There are channels to ask questions and discuss the lectures, homework assignments, and final projects. The channels will be monitored by the TA during class. We will also use Slack for all announcements, so it is important that you are signed up. Feel free to ask questions during class, or anytime."
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "Office hours",
    "text": "Office hours\nOffice hours will be announced during the first week of class for each term."
  },
  {
    "objectID": "index.html#grades",
    "href": "index.html#grades",
    "title": "Advanced Data Science: Welcome and Syllabus",
    "section": "Grades",
    "text": "Grades\n\nParticipation: 10%\nHomework: 60%\nPresentation(s): 30%\n\nIf not specified, assume all work is to be individual: students can discuss projects, including with the TA, but should not work together.\n\nHomework\nHomework will be submitted using git/GitHub - the last commit before midnight will be used to grade the assignment.\n\n\nCollaboration Policy\nYou are welcome and encouraged to discuss the lectures and homework problems with others in order to better understand it, but the work you turn in must be your own. For example, you must write your own code, run your own data analyses, and communicate and explain the results in your own words and with your own visualizations. You may not submit the same or similar work to this course that you have submitted or will submit to another. All students turning in plagiarized solutions will be reported to Office of Academic Integrity, and will fail the assignment.\n\nCollaboration with AI\nWe fully encourage you to use AI when you feel necessary, and to use as an aid. At the end of the course, however, you will be expected to understand the material and be able to do things on your own. That includes pop quizzes, and other assessments that may not be used with AI.\n\n\n\nQuoting Sources\nYou must acknowledge any source code that was not written by you by mentioning the original author(s) directly in your source code (comment or header). You can also acknowledge sources in a README.txt file if you used whole classes or libraries. Do not remove any original copyright notices and headers. However, you are encouraged to use libraries, unless explicitly stated otherwise!\nYou may use examples you find on the web as a starting point, provided its license allows you to re-use it. You must quote the source using proper citations (author, year, title, time accessed, URL) both in the source code and in any publicly visible material. You may not use existing complex combinations or large examples. For example, you may not use a ready to use multiple linked view visualization. You may use parts out of such examples.\n\n\nMissed Activities and Assignment Deadlines\nProjects and homework must be turned in on time, with the exception of late days for homeworks as stated below. It is important that everybody attends and proactively participates in class and online. We understand, however, that certain factors may occasionally interfere with your ability to participate or to hand in work on time. If that factor is an extenuating circumstance, we will ask you to provide documentation directly issued by the University, and we will try to work out an agreeable solution with you (and/or your teammates).\n\n\nLate Day Policy\nEach student is given one late day for homework at the beginning of each term (711 and 712). A late day extends the individual homework deadline by 24 hours without penalty. The late day is intended to give you flexibility: you can use it for any reason no questions asked. You do not get any bonus points for not using your late day in each term. Also, you can only use a late day for the homework deadlines.\nAlthough the each student is only given a total of 1 late day, we will be accepting homework from students that pass this limit. However, we will be deducting 10% for each extra late day. For example, if you have already used your late day for the term, we will deduct 10% for the assignment that is &lt;24 hours late, and 20% points for the assignment that is 24-48 hours late.\n\n\nRegrading Policy\nIt is very important to us that all assignments are properly graded. If you believe there is an error in your assignment grading, please send an email to one of the instructors within 7 days of receiving the grade. No re-grade requests will be accepted orally, and no regrade requests will be accepted more than 7 days after you receive the grade for the assignment."
  },
  {
    "objectID": "exercises/linux_shell.html",
    "href": "exercises/linux_shell.html",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "",
    "text": "Tip: Unless stated otherwise, exercises assume a CSV named penguins.csv (with a header) in the working directory. Exercise 0 shows how to download one from the internet. Answers are hidden—click to reveal.\nIf you do not have a Linux/Unix-based Machine (aka Windows), you can go to GitHub codespaces for one of your repositories and navigate through there."
  },
  {
    "objectID": "exercises/linux_shell.html#download-a-csv-from-the-internet-and-name-it-penguins.csv",
    "href": "exercises/linux_shell.html#download-a-csv-from-the-internet-and-name-it-penguins.csv",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "0) Download a CSV from the internet (and name it penguins.csv)",
    "text": "0) Download a CSV from the internet (and name it penguins.csv)\nQuestion. Use a command-line tool to download a CSV and save it as penguins.csv. Verify it looks like a CSV and preview the first few lines.\nThe file is available at https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n\nShow solution\n\n# Using curl (follow redirects, write to file):\ncurl -L -o penguins.csv \\\n  https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n# Or using wget:\nwget -O penguins.csv \\\n  https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\n\n# (Optional) R from shell:\nR -q -e \"download.file('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv','penguins.csv', mode='wb')\"\n\n# (Optional) Python from shell:\npython - &lt;&lt;'PY'\nimport urllib.request\nurl='https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv'\nurllib.request.urlretrieve(url, 'penguins.csv')\nPY\n\n# Basic checks\nfile penguins.csv\nhead -n 5 penguins.csv\nwc -l penguins.csv   # total lines (incl. header)\nNotes: - -L (curl) follows redirects. -O/-o choose output filename. - Use head, wc -l, and cut -d',' -f1-5 | head to quickly sanity-check."
  },
  {
    "objectID": "exercises/linux_shell.html#where-am-i-whats-here",
    "href": "exercises/linux_shell.html#where-am-i-whats-here",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "1) Where am I? What’s here?",
    "text": "1) Where am I? What’s here?\nQuestion. Print your current directory and list files with sizes and hidden entries.\n\n\nShow solution\n\npwd\nls -lha"
  },
  {
    "objectID": "exercises/linux_shell.html#create-a-working-area",
    "href": "exercises/linux_shell.html#create-a-working-area",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "2) Create a working area",
    "text": "2) Create a working area\nQuestion. Make a folder shell_practice and change into it. Create notes.md.\n\n\nShow solution\n\nmkdir -p shell_practice && cd shell_practice\n: &gt; notes.md   # or: touch notes.md"
  },
  {
    "objectID": "exercises/linux_shell.html#count-rows-in-an-uncompressed-csv-skip-header",
    "href": "exercises/linux_shell.html#count-rows-in-an-uncompressed-csv-skip-header",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "3) Count rows in an uncompressed CSV (skip header)",
    "text": "3) Count rows in an uncompressed CSV (skip header)\nQuestion. Count the number of data rows (exclude the header line) in penguins.csv.\n\n\nShow solution\n\n# total lines minus header\ntotal=$(wc -l &lt; penguins.csv)\necho $(( total - 1 ))\n\n# or with tail:\ntail -n +2 penguins.csv | wc -l"
  },
  {
    "objectID": "exercises/linux_shell.html#compress-a-csv-with-gzip-and-pigz",
    "href": "exercises/linux_shell.html#compress-a-csv-with-gzip-and-pigz",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "4) Compress a CSV with gzip and pigz",
    "text": "4) Compress a CSV with gzip and pigz\nQuestion. Create penguins.csv.gz using (a) gzip and (b) pigz. Compare time and file size.\n\n\nShow solution\n\n# (a) Using gzip\ntime gzip -kf penguins.csv     # -k keep original, -f overwrite\nls -lh penguins.csv penguins.csv.gz\n\n# (b) Using pigz (parallel gzip)\n# If missing, install via your package manager (e.g., apt, brew, conda).\ntime pigz -kf penguins.csv\nls -lh penguins.csv penguins.csv.gz\n\n# Inspect compressed vs uncompressed byte counts\ngzip -l penguins.csv.gz\nNotes: - pigz uses multiple cores → faster on large files; compression ratio is the same algorithm as gzip."
  },
  {
    "objectID": "exercises/linux_shell.html#count-rows-in-a-compressed-csv-.gz",
    "href": "exercises/linux_shell.html#count-rows-in-a-compressed-csv-.gz",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "5) Count rows in a compressed CSV (.gz)",
    "text": "5) Count rows in a compressed CSV (.gz)\nQuestion. Count data rows in penguins.csv.gz without fully decompressing to disk.\n\n\nShow solution\n\n# Using gzip’s decompressor:\ngzip -cd penguins.csv.gz | tail -n +2 | wc -l\n\n# Using pigz if available:\npigz -dc penguins.csv.gz | tail -n +2 | wc -l\n\n# Using zcat (often symlinked to gzip -cd):\nzcat penguins.csv.gz | tail -n +2 | wc -l\nWhy: -c writes to stdout; -d decompresses. tail -n +2 skips the header."
  },
  {
    "objectID": "exercises/linux_shell.html#quick-column-exploration-with-cut-head-sort-uniq",
    "href": "exercises/linux_shell.html#quick-column-exploration-with-cut-head-sort-uniq",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "6) Quick column exploration with cut, head, sort, uniq",
    "text": "6) Quick column exploration with cut, head, sort, uniq\nQuestion. Show the first 5 IDs and the distinct sex values with counts.\n\n\nShow solution\n\nhead -n 6 penguins.csv | cut -d',' -f1      # header + first 5 IDs\ncut -d',' -f2 penguins.csv | tail -n +2 | sort | uniq -c"
  },
  {
    "objectID": "exercises/linux_shell.html#filter-rows-by-a-condition-with-awk",
    "href": "exercises/linux_shell.html#filter-rows-by-a-condition-with-awk",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "7) Filter rows by a condition with awk",
    "text": "7) Filter rows by a condition with awk\nQuestion. Count participants with age &gt;= 60. Compute the mean BMI.\n\n\nShow solution\n\n# age &gt;= 60 (age is 3rd column)\nawk -F',' 'NR&gt;1 && $3 &gt;= 60 {c++} END{print c+0}' penguins.csv\n\n# mean BMI (4th column)\nawk -F',' 'NR&gt;1 {s+=$4; n++} END{print s/n}' penguins.csv"
  },
  {
    "objectID": "exercises/linux_shell.html#find-rows-with-missing-values-in-any-field",
    "href": "exercises/linux_shell.html#find-rows-with-missing-values-in-any-field",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "8) Find rows with missing values in any field",
    "text": "8) Find rows with missing values in any field\nQuestion. Count how many data rows contain an empty field.\n\n\nShow solution\n\n# simple heuristic: consecutive delimiters or trailing comma\ngrep -E ',,' penguins.csv | wc -l\n# more thorough (detect empty at start/end or middle):\nawk -F',' 'NR&gt;1{for(i=1;i&lt;=NF;i++) if($i==\"\"){m++ ; break}} END{print m+0}' penguins.csv"
  },
  {
    "objectID": "exercises/linux_shell.html#save-the-first-20-ids-to-a-file",
    "href": "exercises/linux_shell.html#save-the-first-20-ids-to-a-file",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "9) Save the first 20 IDs to a file",
    "text": "9) Save the first 20 IDs to a file\nQuestion. Write the first 20 IDs (not including header) to sample_ids.txt.\n\n\nShow solution\n\ntail -n +2 penguins.csv | cut -d',' -f1 | head -n 20 &gt; sample_ids.txt\nwc -l sample_ids.txt   # should be 20"
  },
  {
    "objectID": "exercises/linux_shell.html#chain-operations-with-pipes",
    "href": "exercises/linux_shell.html#chain-operations-with-pipes",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "10) Chain operations with pipes",
    "text": "10) Chain operations with pipes\nQuestion. Among male participants, show counts by age (3rd column) in ascending order.\n\n\nShow solution\n\nawk -F',' 'NR&gt;1 && $2==\"male\"{print $3}' penguins.csv | sort -n | uniq -c"
  },
  {
    "objectID": "exercises/linux_shell.html#make-the-analysis-reproducible-with-a-script",
    "href": "exercises/linux_shell.html#make-the-analysis-reproducible-with-a-script",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "11) Make the analysis reproducible with a script",
    "text": "11) Make the analysis reproducible with a script\nQuestion. Create analyze.sh that prints: total rows, rows age ≥ 60, and mean BMI. Run it.\n\n\nShow solution\n\ncat &gt; analyze.sh &lt;&lt; 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\n\ncsv=\"${1:-penguins.csv}\"\n\necho \"File: $csv\"\necho -n \"Total data rows: \"\ntail -n +2 \"$csv\" | wc -l\n\necho -n \"Age &gt;= 60 rows: \"\nawk -F',' 'NR&gt;1 && $3 &gt;= 60 {c++} END{print c+0}' \"$csv\"\n\necho -n \"Mean BMI: \"\nawk -F',' 'NR&gt;1 {s+=$4; n++} END{print s/n}' \"$csv\"\nEOF\n\nchmod +x analyze.sh\n./analyze.sh penguins.csv"
  },
  {
    "objectID": "exercises/linux_shell.html#script-for-compressed-input",
    "href": "exercises/linux_shell.html#script-for-compressed-input",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "12) Script for compressed input",
    "text": "12) Script for compressed input\nQuestion. Modify your script so it also accepts penguins.csv.gz seamlessly.\n\n\nShow solution\n\ncat &gt; analyze_any.sh &lt;&lt; 'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\nf=\"${1:-penguins.csv}\"\n\nstream() {\n  case \"$f\" in\n    *.gz)  gzip -cd \"$f\" ;;\n    *)     cat \"$f\" ;;\n  esac\n}\n\n# Skip header once:\ndata=\"$(stream | tail -n +2)\"\n\necho \"File: $f\"\necho \"Total data rows: $(printf \"%s\\n\" \"$data\" | wc -l)\"\necho \"Age &gt;= 60 rows: $(printf \"%s\\n\" \"$data\" | awk -F',' '$3&gt;=60{c++} END{print c+0}')\"\necho \"Mean BMI: $(printf \"%s\\n\" \"$data\" | awk -F',' '{s+=$4; n++} END{print s/n}')\"\nEOF\n\nchmod +x analyze_any.sh\n./analyze_any.sh penguins.csv.gz"
  },
  {
    "objectID": "exercises/linux_shell.html#record-a-reproducible-terminal-session",
    "href": "exercises/linux_shell.html#record-a-reproducible-terminal-session",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "13) Record a reproducible terminal session",
    "text": "13) Record a reproducible terminal session\nQuestion. Record your workflow to session.log and preview it.\n\n\nShow solution\n\nscript -q session.log\n# …run a few commands…\nexit\nless session.log"
  },
  {
    "objectID": "exercises/linux_shell.html#one-liners-for-large-files",
    "href": "exercises/linux_shell.html#one-liners-for-large-files",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "14) One-liners for large files",
    "text": "14) One-liners for large files\nQuestion. Show the uncompressed byte size of penguins.csv.gz without fully inflating it; then estimate memory needed to load the CSV.\n\n\nShow solution\n\n# Uncompressed and compressed sizes (bytes):\ngzip -l penguins.csv.gz\n\n# Rough row count without header (streaming):\nrows=$(gzip -cd penguins.csv.gz | tail -n +2 | wc -l)\necho \"Rows: $rows\"\nNotes: - gzip -l reports compressed and uncompressed sizes; not row count. - Memory needs depend on parsing overhead; this is only an order-of-magnitude check."
  },
  {
    "objectID": "exercises/linux_shell.html#remotehpc-touchpoint-optional",
    "href": "exercises/linux_shell.html#remotehpc-touchpoint-optional",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "15) Remote/HPC touchpoint (optional)",
    "text": "15) Remote/HPC touchpoint (optional)\nQuestion. Copy your CSV to a remote machine and check its line count there.\n\n\nShow solution\n\nscp penguins.csv user@server:~/data/\nssh user@server 'wc -l ~/data/penguins.csv'"
  },
  {
    "objectID": "exercises/linux_shell.html#parallel-compression-benchmarking-optional-larger-files",
    "href": "exercises/linux_shell.html#parallel-compression-benchmarking-optional-larger-files",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "16) Parallel compression benchmarking (optional, larger files)",
    "text": "16) Parallel compression benchmarking (optional, larger files)\nQuestion. Compare wall-clock time for gzip vs pigz on a large file.\n\n\nShow solution\n\n# Create a larger file by duplication (demo only):\nawk 'NR==1 || FNR&gt;1' penguins.csv penguins.csv penguins.csv penguins.csv penguins.csv \\\n  &gt; big.csv   # header from first file, rest skip header\n\n# Benchmark (prints elapsed time)\n/usr/bin/time -f \"gzip: %E\" gzip -kf big.csv\n/usr/bin/time -f \"pigz: %E\" pigz -kf big.csv\nls -lh big.csv big.csv.gz"
  },
  {
    "objectID": "exercises/linux_shell.html#sanity-checks-and-integrity",
    "href": "exercises/linux_shell.html#sanity-checks-and-integrity",
    "title": "Linux & the Shell — Open-Ended Exercises",
    "section": "17) Sanity checks and integrity",
    "text": "17) Sanity checks and integrity\nQuestion. Verify that the compressed and uncompressed files have identical content checksums.\n\n\nShow solution\n\nmd5sum penguins.csv\ngzip -c penguins.csv | md5sum       # checksum of compressed stream (different)\ngzip -cd penguins.csv.gz | md5sum   # checksum of decompressed content stream\n# To compare content equality:\nmd5sum penguins.csv &gt; a.md5\ngzip -cd penguins.csv.gz | md5sum &gt; b.md5\ndiff a.md5 b.md5   # no output =&gt; identical content\nExplanation: the compressed file’s checksum differs, but the decompressed content checksum should match the original."
  },
  {
    "objectID": "homework/nhanes_project.html",
    "href": "homework/nhanes_project.html",
    "title": "NHANES Project",
    "section": "",
    "text": "The assignment link is here: https://classroom.github.com/a/IqJYjsdq\nThe National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation."
  },
  {
    "objectID": "homework/nhanes_project.html#nhanes-data",
    "href": "homework/nhanes_project.html#nhanes-data",
    "title": "NHANES Project",
    "section": "",
    "text": "The assignment link is here: https://classroom.github.com/a/IqJYjsdq\nThe National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation."
  },
  {
    "objectID": "homework/nhanes_project.html#project",
    "href": "homework/nhanes_project.html#project",
    "title": "NHANES Project",
    "section": "Project",
    "text": "Project\nWe want to demonstrate the skills to connect to the cluster, download data, and perform an analysis. The expected output is a report to be given to a collaborator. Code should be hidden unless otherwise specified. This indicates we should have a statement of the purpose of the analysis/introduction, a description of the data (not using names from code, unless relevant), a description of the methods used (including any model formulations if models are used), the results of the analysis. The results section should reference figures or tables for the analysis and provide insight to what you are seeing. Tell the reader what you are seeing. Then a conclusion/discussion section should provide some distillation of the analysis and what was learned.\n\nThe Data\nThe data in this project is from Wave I (2015-2016) data from the NHANES. Each data set has an online associated codebook. You will likely need to reference these codebooks to understand the data. For example, https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Questionnaire&CycleBeginYear=2015 will list out some of the questionnaires.\n\n\nPurpose\nWe would like to show a few concepts:\n\nConnecting to the Cluster\nInteracting with a Database using SQL, dplyr + dbplyr, or Python equivalents\nPerform an analysis on real-world data, with missingness patterns\n\n\n\nAnalysis\nWe would like find associations of smoking, alcohol use, diabetes, and oral health.\nWe define:\n\nAlcohol use (ALQ101): as Had at least 12 alcohol drinks/1 year.\nDiabetes status (DIQ010): Doctor told you have diabetes (group borderline as yes)\nEver smoker (SMQ020): Smoked at least 100 cigarettes in life?\nOral health issue: (OHAREC) if you are recommended care other than “Continue your regular routine care”\n\n\n\nTasks\n\nName your analysis code file as index.Rmd/index.qmd. The output should be index.pdf. The PDF should be generated from the Rmd/qmd.\n(code should be demonstrated here). Automatically download the nhanes_wave_i.sqlite database from the cluster at /users/bcaffo/dataPublic/nhanes on the JHPCE computing cluster.\n\nInclude the sqlite file in your GitHub (it is publicly available data)\nPut the file in a data/ subfolder (this will be checked).\n\n(code demonstrated here) Connect to the SQLITE database using the DBI/dplyr/dbplyr packages (in R) or equivalent in Python. Create counts of missing values for diabetes status and alcohol use using SQL commands.\nCreate a “table 1” that describes the population, including, age, gender, education level, smoking status, alcohol use, diabetes status, and oral health issues. Decide how to present missing data.\n\nMake sure you label variables appropriately for a collaborator.\n\nCreate a table of smoking status versus whether the person was told benefit giving up cigarettes (from (OHQ_I). Discuss missingness and the relationship of these 2 variables with a hypothesis test.\nWe want to determine the effect of smoking and alcohol use on good oral health (outcome). We would adjust for any demographics you think relevant/significant. Discuss how a model was chosen and any performance metrics. Decide whether to adjust for diabetes and discuss why.\nCreate one figure that demonstrates the effect of age on good oral health.\n\n\n\nNotes\nDo not use setwd in your scripts.\nMake sure the figure aspect ratios are appropriate (get someone else to look it over!).\nAny questions on this project should be posted as an issue on https://github.com/jh-adv-data-sci/adv_data_sci_2023/issues."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Class 1: Intro and Linux & Shell – Present (intro, basics)\n\nClass 2: Linux & Shell - Practice (hands-on commands)\n\n\n\n\n\nHomework 1: Homework 1\n\n\n\n\n\nClass 3: SLURM and the Cluster\nClass 4: SLURM and the Cluster - Practice\n\n\n\n\n\nClass 5: Power Lecture (foundations/tools)\n\nClass 6: Power – Practice\n\n\n\n\n\nClass 7: R packages\nClass 8: Grants\n\n\n\n\n\nClass 9: Present P1 – Do (SAP style project presentation)\n\nClass 10: SQL Data Lecture – Do (scientific data concepts)\n\n\n\n\n\nClass 11: Data Visualization – Lecture\n\nClass 12: Data Visualization (EDA) – Do\n\n\n\n\n\nClass 13: App Dashboard – Development (setup & workflow)\n\nClass 14: App Dashboard – Do (hands-on build)\n\n\n\n\n\nClass 15: APIs + Pulling\nClass 16: Final Presentations\n\n\n\n\n\n\n\n\nClass 1: Language Models – API (overview, usage)\n\nClass 2: Language Models – Do (hands-on, coding)\n\n\n\n\n\nClass 3: Performance Metrics Measurement & Models – Lecture\n\nClass 4: Predictive Competition – Present (framing a comp)\n\n\n\n\n\nClass 5: Model Selection – tidymodels (intro)\n\nClass 6: Model Selection – Do (practice)\n\n\n\n\n\nClass 7: Missing Data\nClass 8: Missing Data – Do (coding exercises)\n\n\n\n\n\nClass 9: Pipelining Techniques – Lecture\n\nClass 10: Pipelining Techniques – Do\n\n\n\n\n\nClass 11: Performance Metrics Measurement & Models – Lecture\n\nClass 12: Measurement & Models – Do\n\n\n\n\n\nClass 13: Practical Bayes – Lecture (methods)\n\nClass 14: Practical Bayes – Do (applications)\n\n\n\n\n\nClass 15: Presentation prep\n\nClass 16: Final Presentations"
  },
  {
    "objectID": "schedule.html#advanced-data-science-i",
    "href": "schedule.html#advanced-data-science-i",
    "title": "Schedule",
    "section": "",
    "text": "Class 1: Intro and Linux & Shell – Present (intro, basics)\n\nClass 2: Linux & Shell - Practice (hands-on commands)\n\n\n\n\n\nHomework 1: Homework 1\n\n\n\n\n\nClass 3: SLURM and the Cluster\nClass 4: SLURM and the Cluster - Practice\n\n\n\n\n\nClass 5: Power Lecture (foundations/tools)\n\nClass 6: Power – Practice\n\n\n\n\n\nClass 7: R packages\nClass 8: Grants\n\n\n\n\n\nClass 9: Present P1 – Do (SAP style project presentation)\n\nClass 10: SQL Data Lecture – Do (scientific data concepts)\n\n\n\n\n\nClass 11: Data Visualization – Lecture\n\nClass 12: Data Visualization (EDA) – Do\n\n\n\n\n\nClass 13: App Dashboard – Development (setup & workflow)\n\nClass 14: App Dashboard – Do (hands-on build)\n\n\n\n\n\nClass 15: APIs + Pulling\nClass 16: Final Presentations"
  },
  {
    "objectID": "schedule.html#advanced-data-science-ii",
    "href": "schedule.html#advanced-data-science-ii",
    "title": "Schedule",
    "section": "",
    "text": "Class 1: Language Models – API (overview, usage)\n\nClass 2: Language Models – Do (hands-on, coding)\n\n\n\n\n\nClass 3: Performance Metrics Measurement & Models – Lecture\n\nClass 4: Predictive Competition – Present (framing a comp)\n\n\n\n\n\nClass 5: Model Selection – tidymodels (intro)\n\nClass 6: Model Selection – Do (practice)\n\n\n\n\n\nClass 7: Missing Data\nClass 8: Missing Data – Do (coding exercises)\n\n\n\n\n\nClass 9: Pipelining Techniques – Lecture\n\nClass 10: Pipelining Techniques – Do\n\n\n\n\n\nClass 11: Performance Metrics Measurement & Models – Lecture\n\nClass 12: Measurement & Models – Do\n\n\n\n\n\nClass 13: Practical Bayes – Lecture (methods)\n\nClass 14: Practical Bayes – Do (applications)\n\n\n\n\n\nClass 15: Presentation prep\n\nClass 16: Final Presentations"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Linux & the Shell — Open-Ended Exercises\n\n\nHow to Effectively Work at the Command Line\n\n\n\n\n\n\n\n\n01-Linux & Shell\n\n\n\n\n\n\n\n\n\n\n\n\nSubmitting Jobs on HPC with SLURM\n\n\nHow to Effectively Work on JHPCE\n\n\n\n\n\n\n\n\n02-SLURM and the Cluster\n\n\n\n\n\n\n\n\n\n\n\n\nPower & Sample Size\n\n\nEstimating Sample Size and Power from Parameters\n\n\n\n\n\n\n\n\n03-Power and Sample Size\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exercises/power.html",
    "href": "exercises/power.html",
    "title": "Power & Sample Size",
    "section": "",
    "text": "Conventions: Two‑sided tests at α = 0.05 unless stated. Use R where helpful. Symbols: δ = effect size in the outcome scale; σ = SD; n = total sample unless noted. For two‑sample tests, assume equal allocation and equal variances unless stated."
  },
  {
    "objectID": "exercises/power.html#onesample-ttest-power-n-10-minutes",
    "href": "exercises/power.html#onesample-ttest-power-n-10-minutes",
    "title": "Power & Sample Size",
    "section": "1) One‑sample t‑test power & n (10 minutes)",
    "text": "1) One‑sample t‑test power & n (10 minutes)\nSetup. You expect observations (\\(Y\\sim \\mathcal{N}(\\mu,\\sigma^2)\\)) with (\\(\\sigma=10\\)). You will test (\\(H_0: \\mu=0\\)) vs (\\(H_1: \\mu\\neq 0\\)). The scientifically relevant effect is (\\(\\delta = 3\\)) (i.e., (\\(\\mu=3\\))).\n1A. Compute power for (\\(n=50\\)).\n\n\nAnswer\n\nAnalytic normal approx: (\\(Z = \\bar Y/(\\sigma/\\sqrt n)\\)), noncentrality (\\(\\lambda = \\delta\\sqrt n/\\sigma\\)).\n[ = 3/10 = 3/10 . ]\nTwo‑sided power (z‑approx): [ 1 - (z_{0.975}-) + (-z_{0.975}-) -(1.96-2.121)+(-1.96-2.121) -(-0.161)+(-4.081). ] This is (-(0.436)+ ). Using exact t power:\n\n\nCode\npower.t.test(n = 50, delta = 3, sd = 10, sig.level = 0.05,\n             type = \"one.sample\", alternative = \"two.sided\")$power\n\n\n\n1B. Required (n) for 80% power.\n\n\nAnswer\n\nClosed‑form (z‑approx): ( n (z_{0.975}+z_{0.8})2,/^2 ).\n[ n (1.96+0.84)^2 = (2.80)^2 /9 . ]\nExact via R:\n\n\nCode\npower.t.test(power = 0.8, delta = 3, sd = 10, sig.level = 0.05,\n             type = \"one.sample\", alternative = \"two.sided\")$n"
  },
  {
    "objectID": "exercises/power.html#twosample-ttest-equal-variance-8-minutes",
    "href": "exercises/power.html#twosample-ttest-equal-variance-8-minutes",
    "title": "Power & Sample Size",
    "section": "2) Two‑sample t‑test (equal variance) (8 minutes)",
    "text": "2) Two‑sample t‑test (equal variance) (8 minutes)\nSetup. Two arms, equal allocation, common (), difference of means (= 5).\n2A. Per‑group (n) for 90% power.\n\n\nAnswer\n\nZ‑approx per‑group (n): ( n (z_{0.975}+z_{0.90})2,/^2 ).\n[ n (1.96+1.282)2(122)/5^2 = 2(3.242)^2/25 . ] So about 122 per group. Exact:\n\n\nCode\npower.t.test(power = 0.9, delta = 5, sd = 12, sig.level = 0.05,\n             type = \"two.sample\", alternative = \"two.sided\")$n\n\n\n\n2B. Power if (n=40) per group.\n\n\nAnswer\n\n\n\nCode\npower.t.test(n = 40, delta = 5, sd = 12, sig.level = 0.05,\n             type = \"two.sample\", alternative = \"two.sided\")$power"
  },
  {
    "objectID": "exercises/power.html#paired-ttest-5-minutes",
    "href": "exercises/power.html#paired-ttest-5-minutes",
    "title": "Power & Sample Size",
    "section": "3) Paired t‑test (5 minutes)",
    "text": "3) Paired t‑test (5 minutes)\nSetup. Pre/post measurements with (=*=8), correlation (). Mean change ().\nQuestion. Required number of pairs for 80% power.\n\n\nAnswer\n\nSD of differences: (_D== = 8=8.) Use one‑sample t on differences with (sd=_D), effect ():\n\n\nCode\npower.t.test(power = 0.8, delta = 2, sd = 7.155, sig.level = 0.05,\n             type = \"one.sample\", alternative = \"two.sided\")$n"
  },
  {
    "objectID": "exercises/power.html#simple-linear-regression-test-a-slope-10-minutes",
    "href": "exercises/power.html#simple-linear-regression-test-a-slope-10-minutes",
    "title": "Power & Sample Size",
    "section": "4) Simple linear regression: test a slope (10 minutes)",
    "text": "4) Simple linear regression: test a slope (10 minutes)\nSetup. Model (Y=_0+_1 X+), (N(0,^2)) with () and (X) standardized: (X), ((X)=1). Suppose (_1=3). Test (H_0:_1=0).\n4A. Power at (n=60) using the noncentral t formulation.\n\n\nAnswer\n\nFor centered/standardized (X), (S_{xx}=(x_i-x)^2n-1). The test statistic for (_1) has noncentrality [ = = 0.3. ] Power (two‑sided) is (1-) with df=(n-2=58):\n\n\nCode\nn &lt;- 60; sigma &lt;- 10; beta1 &lt;- 3\nlambda &lt;- beta1/sigma * sqrt(n-1)\nalpha &lt;- 0.05; df &lt;- n - 2\nc &lt;- qt(1 - alpha/2, df)\n# power = P(|T_nc| &gt; c)\n1 - (pt(c, df, ncp=lambda) - pt(-c, df, ncp=lambda))\n\n\n\n4B. Alternative via correlation. Show equivalence.\n\n\nAnswer\n\nFor SLR, (r=(X,Y)=_1_X/_Y). With (_X=1), (_Y===), so (r/10.44.) Test of (r=0) uses the same df and noncentrality (,r/), which equals () above.\n\n\nCode\nr &lt;- 3/sqrt(9+100)\nlambda2 &lt;- sqrt(n-2) * r / sqrt(1 - r^2)\nall.equal(lambda, lambda2)"
  },
  {
    "objectID": "exercises/power.html#multiple-regression-overall-r2-7-minutes",
    "href": "exercises/power.html#multiple-regression-overall-r2-7-minutes",
    "title": "Power & Sample Size",
    "section": "5) Multiple regression: overall (R^2) (7 minutes)",
    "text": "5) Multiple regression: overall (R^2) (7 minutes)\nSetup. You plan (p=4) predictors and expect population (R^2=0.20). Test (H_0: R^2=0) (overall model). Find power at (n=80) and required (n) for 80% power.\n\n\nAnswer\n\nUse Cohen’s (f2=R2/(1-R^2)=0.25). The overall F test has df1=(p), df2=(n-p-1), and noncentrality (=f^2(n-p-1)).\nPower at (n=80):\n\n\nCode\np &lt;- 4; R2 &lt;- 0.20; f2 &lt;- R2/(1-R2)\nn &lt;- 80; df1 &lt;- p; df2 &lt;- n - p - 1\nlambda &lt;- f2 * df2\nalpha &lt;- 0.05; Fc &lt;- qf(1 - alpha, df1, df2)\n# P(F_nc &gt; Fc)\npower &lt;- 1 - pf(Fc, df1, df2, ncp=lambda)\npower\n\n\nSolve for (n) (80% power):\n\n\nCode\nf_target &lt;- function(n){\n  df2 &lt;- n - p - 1\n  if (df2 &lt;= 0) return(-1)  # invalid\n  Fc &lt;- qf(0.95, df1=p, df2=df2)\n  1 - pf(Fc, df1=p, df2=df2, ncp=f2*df2) - 0.80\n}\nuniroot(f_target, c(p+5, 1000))$root"
  },
  {
    "objectID": "exercises/power.html#sample-size-for-prediction-interval-width-in-slr-10-minutes",
    "href": "exercises/power.html#sample-size-for-prediction-interval-width-in-slr-10-minutes",
    "title": "Power & Sample Size",
    "section": "6) Sample size for prediction interval width in SLR (10 minutes)",
    "text": "6) Sample size for prediction interval width in SLR (10 minutes)\nSetup. In (Y=_0+_1 X+) with (N(0,^2)), suppose () and you plan (X) standardized (mean 0, var 1). You want a 95% prediction interval for a future observation at (x_0) to have total width (W). Consider two cases: (i) (x_0=0) (mean of (X)), (ii) (x_0=1) (1 SD from mean).\nFacts. The half‑width (HW) at (x_0) is [ (x_0) = t_{0.975,,n-2},,,S_{xx}=(x_i-x)^2n-1. ] The minimum achievable half‑width is as (n): (t_{,0.975},,).\n6A. Feasibility check. With (), what is the smallest possible total width (W_{min})?\n\n\nAnswer\n\n(W_{min}=2 .) Any target width (W&lt;39.2) is impossible regardless of (n).\n\n6B. Find (n) to achieve (W=48) (half‑width 24) at (x_0=0).\n\n\nAnswer\n\nSolve ( t_{0.975,n-2},10, = 24). Use numeric root‑finding:\n\n\nCode\nsigma &lt;- 10; HW &lt;- 24\nf &lt;- function(n){\n  df &lt;- n - 2\n  if (df &lt;= 0) return(1e6)\n  t &lt;- qt(0.975, df)\n  t * sigma * sqrt(1 + 1/n) - HW\n}\nceiling(uniroot(f, c(5, 1e5))$root)\n\n\n\n6C. Repeat for (x_0=1). Use (S_{xx}n-1), (x).\n\n\nAnswer\n\nHalf‑width equation: ( t, = 24 ).\n\n\nCode\nsigma &lt;- 10; HW &lt;- 24\nf &lt;- function(n){\n  df &lt;- n - 2\n  if (df &lt;= 2) return(1e6)\n  t &lt;- qt(0.975, df)\n  Sxx &lt;- n - 1\n  t * sigma * sqrt(1 + 1/n + 1/Sxx) - HW\n}\nceiling(uniroot(f, c(6, 1e6))$root)\n\n\nNote the larger (n) due to being 1 SD away from the design mean.\n\n6D. (Optional) Design‑sensitive planning. If you can spread (X) to increase (S_{xx}) (e.g., choose equally spaced design over a range), how does that change (n) for fixed (W)?\n\n\nAnswer\n\nIncreasing (S_{xx}) shrinks the term ((x_0-x)^2/S_{xx}), reducing the half‑width for off‑mean predictions. For a fixed (n), wider X range lowers HW at (x_0x). Conversely, for fixed HW, larger (S_{xx}) allows smaller (n)."
  },
  {
    "objectID": "exercises/power.html#optional-monte-carlo-verification-5-minutes",
    "href": "exercises/power.html#optional-monte-carlo-verification-5-minutes",
    "title": "Power & Sample Size",
    "section": "7) (Optional) Monte Carlo verification (5 minutes)",
    "text": "7) (Optional) Monte Carlo verification (5 minutes)\nSimulate to verify analytic power for a two‑sample t‑test or slope test.\n\n\nAnswer\n\n\n\nCode\nset.seed(1)\nB &lt;- 5000\nn &lt;- 40; sd &lt;- 12; delta &lt;- 5; alpha &lt;- 0.05\np &lt;- replicate(B, {\n  x1 &lt;- rnorm(n, 0, sd); x2 &lt;- rnorm(n, delta, sd)\n  t.test(x2, x1, var.equal = TRUE)$p.value\n})\nmean(p &lt; alpha)  # ~ power\n\n\n\n\n\nQuick reference\n\nOne‑sample n (z‑approx): ( n = (z_{1-/2}+z_{1-})2,/^2 )\nTwo‑sample per‑group n: ( n = 2(z_{1-/2}+z_{1-})2,/^2 )\nSLR slope power ncp: ( = (_1/) (_1/) )\nOverall MR test: (f2=R2/(1-R^2),; =f^2(n-p-1))\nPI half‑width at (x_0): ( t, )"
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html",
    "href": "exercises/slurm_and_the_cluster.html",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "",
    "text": "Setup assumptions: You have SSH access to the cluster and R is available via modules or a system install. Replace partition/account names with your site’s values."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#warmup-explore-the-cluster",
    "href": "exercises/slurm_and_the_cluster.html#warmup-explore-the-cluster",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Warm‑up: Explore the cluster",
    "text": "Warm‑up: Explore the cluster\nQuestion. What partitions/queues can you use, and what’s the default time limit and CPU/memory policy?\n\n\nShow solution\n\n\n\nCode\n# Partitions and key limits\nsinfo -o '%P %l %c %m %a'   # Partition, time limit, CPUs, memory, availability\n\n# Default Slurm config snippets\nscontrol show config | egrep 'DefMemPerCPU|DefMemPerNode|SchedulerType|SelectType'\n\n# Your account/QoS\nsacctmgr show assoc user=$USER format=Cluster,Account,Partition,MaxWall,MaxCPUs,MaxJobs,Grp* -Pn 2&gt;/dev/null || true\n\n\nNotes: sinfo gives active partitions. scontrol show config reveals defaults like memory policy per CPU or per node.\n\n\n\nModules\nQuestion. What modules are available? Which ones are loaded?\n\n\nCode\nmodule avail\n\n\n\n\nHIPAA\nRead the HIPAA section of https://jhpce.jhu.edu/joinus/hipaa/\n\n\nSSH\nSkim how to set up SSH: https://jhpce.jhu.edu/access/ssh/\n\n\nTransfer one file\nSee https://jhpce.jhu.edu/access/file-transfer/ for transferring files, do this on the transfer node.\n\n\nGet on the interactive node\nUse --partition=interactive. Try to have 2 running."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#submit-a-minimal-r-job-single-task",
    "href": "exercises/slurm_and_the_cluster.html#submit-a-minimal-r-job-single-task",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Submit a minimal R job (single task)",
    "text": "Submit a minimal R job (single task)\nQuestion. Write a Slurm batch script that runs a one‑liner in R and writes to slurm-%j.out. Name the job rmin and have a time limit of 5 minutes, with 2G of memory requested. The one line should just do cat(\"Hello from R on Slurm!\\n\")\n\n\nShow solution\n\n\n\nCode\ncat &gt; r_minimal.sbatch &lt;&lt;'SB'\n#!/usr/bin/env bash\n#SBATCH -J rmin\n#SBATCH -t 00:05:00\n#SBATCH -c 1\n#SBATCH --mem=2G\n#SBATCH -o slurm-%j.out\n\nmodule load conda_R 2&gt;/dev/null || true  # or: module load R; or skip if R in PATH\nRscript -e 'cat(\"Hello from R on Slurm!\\n\")'\nSB\n\nsbatch r_minimal.sbatch\n\n\n%j expands to the JobID. Inspect output with tail -f slurm-&lt;jobid&gt;.out."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#parameterized-simulation-in-r-script",
    "href": "exercises/slurm_and_the_cluster.html#parameterized-simulation-in-r-script",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Parameterized simulation in R (script)",
    "text": "Parameterized simulation in R (script)\nQuestion. Create sim.R that accepts command‑line args: n, mu, sigma, seed, runs a simple simulation (e.g., mean of rnorm), and writes a CSV line to results/sim_&lt;seed&gt;.csv. See ?commandArgs.\n\n\nShow solution\n\n\n\nCode\nmkdir -p results\ncat &gt; sim.R &lt;&lt;'RS'\nargs &lt;- commandArgs(trailingOnly = TRUE)\nstopifnot(length(args) == 4)\n\nn     &lt;- as.integer(args[1])\nmu    &lt;- as.numeric(args[2])\nsigma &lt;- as.numeric(args[3])\nseed  &lt;- as.integer(args[4])\n\nset.seed(seed)\nx &lt;- rnorm(n, mu, sigma)\nres &lt;- data.frame(n=n, mu=mu, sigma=sigma, seed=seed,\n                  mean=mean(x), sd=sd(x))\n\nout &lt;- sprintf('results/sim_%d.csv', seed)\nwrite.csv(res, out, row.names = FALSE)\ncat('Wrote', out, '\\n')\nRS"
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#batch-script-that-passes-parameters-to-r",
    "href": "exercises/slurm_and_the_cluster.html#batch-script-that-passes-parameters-to-r",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Batch script that passes parameters to R",
    "text": "Batch script that passes parameters to R\nQuestion. Write sim.sbatch that runs Rscript sim.R 10000 0 1 42.\n\n\nShow solution\n\n\n\nCode\ncat &gt; sim.sbatch &lt;&lt;'SB'\n#!/usr/bin/env bash\n#SBATCH -J sim\n#SBATCH -t 00:05:00\n#SBATCH -c 1\n#SBATCH --mem=2G\n#SBATCH -o slurm-%j.out\n\nmodule load conda_R 2&gt;/dev/null || true\nRscript sim.R 10000 0 1 42\nSB\n\nsbatch sim.sbatch"
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#use-expand.grid-in-r-with-slurm_array_task_id-no-params.csv",
    "href": "exercises/slurm_and_the_cluster.html#use-expand.grid-in-r-with-slurm_array_task_id-no-params.csv",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Use expand.grid in R with SLURM_ARRAY_TASK_ID (no params.csv)",
    "text": "Use expand.grid in R with SLURM_ARRAY_TASK_ID (no params.csv)\nQuestion. Within R, generate a parameter grid and pick the row corresponding to your task index using SLURM_ARRAY_TASK_ID. Run sim.R with those values.\n\n\nShow solution\n\ndriver.R\n\n\nCode\n# Read the array index (default 1 when running locally)\nidx &lt;- as.integer(Sys.getenv('SLURM_ARRAY_TASK_ID', '1'))\n\n# Define your parameter grid\nparams &lt;- expand.grid(\n  n    = c(1e4, 5e4),\n  mu   = c(0, 0.2),\n  sigma= c(1, 2),\n  seed = 1:8\n)\n\nstopifnot(idx &gt;= 1, idx &lt;= nrow(params))\np &lt;- params[idx, , drop = FALSE]\n\nset.seed(p$seed)\nx &lt;- rnorm(n, mu, sigma)\n\n\n\n\nCode\n# Minimal array submission (match the array range to nrow(params) = 64)\ncat &gt; driver_array.sbatch &lt;&lt;'SB'\n#!/usr/bin/env bash\n#SBATCH -J drv\n#SBATCH -t 00:10:00\n#SBATCH -c 1\n#SBATCH --mem=2G\n#SBATCH -o slurm-%A_%a.out\n#SBATCH --array=1-64\nmodule load conda_R 2&gt;/dev/null || true\nRscript driver.R\nSB\n\nsbatch driver_array.sbatch\n\n\nNotes: - Ensure the array range matches nrow(params) inside driver.R. - Locally, you can test with SLURM_ARRAY_TASK_ID=3 Rscript driver.R."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#array-job-to-run-the-grid",
    "href": "exercises/slurm_and_the_cluster.html#array-job-to-run-the-grid",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Array job to run the grid",
    "text": "Array job to run the grid\nQuestion. Write sim_array.sbatch that runs one row of params.csv per array task and writes logs as slurm-%A_%a.out.\n\n\nShow solution\n\n\n\nCode\ncat &gt; sim_array.sbatch &lt;&lt;'SB'\n#!/usr/bin/env bash\n#SBATCH -J simarr\n#SBATCH -t 00:10:00\n#SBATCH -c 1\n#SBATCH --mem=2G\n#SBATCH -o slurm-%A_%a.out\n#SBATCH --array=1-32           # &lt;-- set to nrow(params.csv)\n\nmodule load conda_R 2&gt;/dev/null || true\n\n# Read the line matching this array index\nIFS=',' read -r n mu sigma seed &lt; &lt;(sed -n \"${SLURM_ARRAY_TASK_ID}p\" params.csv)\n\necho \"Task ${SLURM_ARRAY_TASK_ID}: n=$n mu=$mu sigma=$sigma seed=$seed\"\nRscript sim.R \"$n\" \"$mu\" \"$sigma\" \"$seed\"\nSB\n\n# Submit after matching the array range to your params\nlines=$(wc -l &lt; params.csv)\nsbatch --array=1-$lines sim_array.sbatch\n\n\nKey env vars: SLURM_ARRAY_TASK_ID (index), %A (ArrayJobID), %a (TaskID) for log naming."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#monitor-inspect-and-cancel-jobs",
    "href": "exercises/slurm_and_the_cluster.html#monitor-inspect-and-cancel-jobs",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Monitor, inspect, and cancel jobs",
    "text": "Monitor, inspect, and cancel jobs\nQuestion. How do you check running jobs and see finished job states? Cancel a stuck task 5 of an array.\n\n\nShow solution\n\n\n\nCode\n# Running/queued jobs for you\nsqueue -u $USER -o '%A %j %t %M %D %R'\n\n# Completed job accounting (after finish)\nsacct -u $USER --starttime today \\\n  --format=JobID,JobName%30,State,Elapsed,MaxRSS,ExitCode\n\n# Show details for a specific job\na_jobid=123456\nscontrol show job $a_jobid | less\n\n# Cancel a whole job or a single array task\nscancel 123456           # whole job\nscancel 123456_5         # only task 5\n\n\nUse tail -f slurm-123456_5.out to live‑watch a specific task’s output."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#resource-requests-usage-diagnostics",
    "href": "exercises/slurm_and_the_cluster.html#resource-requests-usage-diagnostics",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Resource requests & usage diagnostics",
    "text": "Resource requests & usage diagnostics\nQuestion. Request 2 CPUs and 4G RAM per task; later, inspect actual usage.\n\n\nShow solution\n\n\n\nCode\n# In your .sbatch header\n#SBATCH -c 2\n#SBATCH --mem=4G\n\n# Inspect after completion\nsacct -j 123456 -o JobID,JobName%30,AllocCPUS,Elapsed,MaxRSS,State,ExitCode\n\n\nTip: Prefer --mem= (per node) vs --mem-per-cpu= depending on your site policy."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#resubmit-only-failed-array-tasks",
    "href": "exercises/slurm_and_the_cluster.html#resubmit-only-failed-array-tasks",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Resubmit only failed array tasks",
    "text": "Resubmit only failed array tasks\nQuestion. Find which tasks failed from a previous array job and resubmit only those indices.\n\n\nShow solution\n\n\n\nCode\njid=123456  # parent array job ID\n# List failed indices (State not COMPLETED)\nfail=$(sacct -j $jid --format=JobID,State -n | awk -F'[_. ]' '$2!=\"batch\" && $3!=\"COMPLETED\" {print $2}' | sort -n | uniq)\n\necho \"Failed indices: $fail\"\n[ -n \"$fail\" ] && sbatch --array=$(echo $fail | tr ' ' ',') sim_array.sbatch\n\n\nThis parses sub‑job entries like 123456_7 and extracts 7 when State != COMPLETED."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#interactive-work-debugging",
    "href": "exercises/slurm_and_the_cluster.html#interactive-work-debugging",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Interactive work (debugging)",
    "text": "Interactive work (debugging)\nQuestion. Start an interactive shell on a compute node and verify R sees multiple threads.\n\n\nShow solution\n\n\n\nCode\n# Allocate and attach to a compute node for 10 min with 2 CPUs and 2G RAM\nsalloc -t 00:10:00 -c 2 --mem=2G\nsrun --pty bash\n\nmodule load conda_R 2&gt;/dev/null || true\nR -q &lt;&lt;'RS'\nparallel::detectCores()\nsessionInfo()\nRS\n\n# Exit when done\nexit  # from R\nexit  # from shell to release allocation"
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#vs-code-positron-remote-dev",
    "href": "exercises/slurm_and_the_cluster.html#vs-code-positron-remote-dev",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "VS Code / Positron remote dev",
    "text": "VS Code / Positron remote dev\nQuestion. Configure VS Code (or Positron) to edit/submit jobs on the cluster via SSH.\n\n\nShow solution\n\nVS Code (Remote - SSH):\n\nInstall extensions: Remote - SSH, R, optionally Python, Bash IDE.\nCreate ~/.ssh/config entry on your laptop:\nHost myhpc\n  HostName login.cluster.edu\n  User your_netid\n  IdentityFile ~/.ssh/id_ed25519\nIn VS Code: Remote Explorer → SSH Targets → myhpc → Connect.\nOpen your home/project directory on the cluster.\nEnsure R is available in PATH on the cluster; set VS Code R extension (if needed) to use /usr/bin/R or your module path.\nUse VS Code terminal (connected to myhpc) to run sbatch, squeue, etc. Edit .sbatch/.R files locally but they execute on the cluster.\n\nPositron:\n\nInstall the Remote - SSH (or built‑in remote) capability; connect similarly to open a remote workspace.\nConfigure the R path in Positron settings to point to the cluster’s R binary; use the integrated terminal for sbatch.\n\nOptional: set up SSH keys and agent forwarding to enable Git from the cluster."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#helpful-shell-aliasesfunctions-for-slurm",
    "href": "exercises/slurm_and_the_cluster.html#helpful-shell-aliasesfunctions-for-slurm",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Helpful shell aliases/functions for SLURM",
    "text": "Helpful shell aliases/functions for SLURM\nTask: Look over these and add helpers to your ~/.bash_profile to speed up common tasks. Better yet, make a ~/.bash_aliases and put this in ~/.bashrc:\n\n\nCode\n# User specific aliases and functions\nif [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\n\n\nCommands:\n\n\nCode\ncat &gt;&gt; ~/.bashrc &lt;&lt;'BRC'\n# Slurm quick views\nalias sj='squeue -u $USER -o \"%A %j %t %M %D %R\"'\nalias sa='sacct -u $USER --starttime today -o JobID,JobName%30,State,Elapsed,MaxRSS,ExitCode'\n\n# Tail latest log(s)\nsl(){ tail -n +1 -f slurm-*.out; }\n\n# Submit and print JobID only\nsb(){ sbatch --parsable \"$@\"; }\n\n# Describe a job\nsd(){ scontrol show job \"$1\" | less; }\n\n# Resubmit failed array tasks for a parent JobID\nsref(){ jid=\"$1\"; idx=$(sacct -j \"$jid\" -n -o JobID,State | awk -F'[_. ]' '$2!=\"batch\" && $3!=\"COMPLETED\"{print $2}' | sort -n | uniq | paste -sd, -); [ -n \"$idx\" ] && sbatch --array=\"$idx\" sim_array.sbatch; }\n\nalias sqme=\"squeue --me\"\n\nRnosave () \n{ \n    x=\"$1\";\n    tempfile=`mktemp file.XXXX.sh`;\n    echo \"#!/bin/bash\" &gt; $tempfile;\n    echo \". ~/.bash_profile\" &gt;&gt; $tempfile;\n    echo \"R --no-save &lt; ${x}\" &gt;&gt; $tempfile;\n    shift;\n    cmd=\"${submitter} $@ $tempfile\";\n    echo \"cmd is $cmd\";\n    ${cmd};\n    rm $tempfile\n}\n\n## Git Add, Commit, Push (GACP)\nfunction gacp { \n    git pull;\n    git add --all .;\n    git commit -m \"${1}\";\n    if [ -n \"${2}\" ]; then\n        echo \"Tagging Commit\";\n        git tag \"${2}\";\n        git push origin \"${2}\";\n        git commit --amend -m \"${1} [ci skip]\";\n    fi;\n    git push origin\n}\n\n## raw ls\nalias rls=\"/usr/bin/ls -f\"\n\n## grep on history this is really important\nfunction hgrep {\n    history | grep \"$@\"\n}\n\nBRC\n\n# Reload shell config\nsource ~/.bashrc\n\n\nThese helpers give you one‑letter shortcuts for listing jobs (sj), recent accounting (sa), tailing logs (sl), describing a job (sd), and resubmitting failures (sref)."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#bonus-make-a-project-scaffold",
    "href": "exercises/slurm_and_the_cluster.html#bonus-make-a-project-scaffold",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Bonus: Make a project scaffold",
    "text": "Bonus: Make a project scaffold\nQuestion. Create a scaffold with directories and template scripts for simulations.\n\n\nShow solution\n\n\n\nCode\nmkdir -p {scripts,results,logs}\n\n# Template sbatch header you can copy into scripts/\ncat &gt; scripts/_header.sbatch &lt;&lt;'H'\n#!/usr/bin/env bash\n#SBATCH -t 00:10:00\n#SBATCH -c 1\n#SBATCH --mem=2G\n#SBATCH -o logs/slurm-%A_%a.out\nH\n\n\nNow copy _header.sbatch into new jobs and append your commands."
  },
  {
    "objectID": "exercises/slurm_and_the_cluster.html#deliverables-for-practice",
    "href": "exercises/slurm_and_the_cluster.html#deliverables-for-practice",
    "title": "Submitting Jobs on HPC with SLURM",
    "section": "Deliverables (for practice)",
    "text": "Deliverables (for practice)\n\nr_minimal.sbatch and output log\nsim.R, params.csv, sim_array.sbatch\nEvidence of a dependency submission (combine.sbatch and combined output)\nYour updated ~/.bashrc helpers"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "NHANES Project\n\n\nAnalyzing NHANES\n\n\n\n\n\n\n\n\n03-NHANES\n\n\n\n\n\nNo matching items"
  }
]